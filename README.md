# Lab 3: Contextual Bandit-Based News Article Recommendation

**Course**: Reinforcement Learning Fundamentals  
**Student**: Sher Partap Singh  
**Roll Number**: U20230081  
**Branch**: `sher_U20230081`

---

## ğŸ“‹ Overview

This project implements a **Contextual Multi-Armed Bandit (CMAB)** system for personalized news article recommendations. The system:
1. Classifies users into categories (User1, User2, User3) based on behavioral features
2. Uses bandit algorithms to learn optimal news category recommendations per user type
3. Samples articles from the selected category to maximize engagement

---

## ğŸ—ï¸ Architecture

```
User Features â†’ [Classifier] â†’ User Context â†’ [Bandit Policy] â†’ News Category â†’ [Sampler] â†’ Article
```

**Problem Formulation:**
- **Contexts**: 3 user types (User1, User2, User3)
- **Arms**: 4 news categories (Entertainment, Education, Tech, Crime) per context
- **Total Arms**: 12 (3 contexts Ã— 4 categories)

---

## ğŸ“Š Results Summary

### User Classification
| Metric | Value |
|--------|-------|
| Model | RandomForestClassifier |
| Train/Val Split | 80/20 |
| Validation Accuracy | **89.75%** |

**Top Features**: `region_code`, `session_duration`, `browsing_depth`, `scroll_activity`, `time_on_site`

### Bandit Algorithm Performance (T=10,000 steps)

| Algorithm | Best Hyperparameter | Average Reward |
|-----------|---------------------|----------------|
| **UCB** | C=0.5 | **7.13** |
| SoftMax | Ï„=1.0 | 7.02 |
| Îµ-Greedy | Îµ=0.01 | 6.62 |

---

## ğŸ”¬ Algorithm Analysis

### Epsilon-Greedy
- Simple exploration-exploitation strategy
- **Îµ=0.01**: Fast convergence, risk of missing optimal arms
- **Îµ=0.1**: Good balance for most scenarios
- **Îµ=0.3**: Extensive exploration, slower learning

### Upper Confidence Bound (UCB)
- Systematic uncertainty-based exploration
- **C=0.5**: More exploitation, faster convergence
- **C=1.0**: Balanced approach
- **C=2.0**: More exploration, better long-term performance
- **Best performer** overall without manual tuning

### SoftMax
- Probabilistic selection via Boltzmann distribution
- **Ï„=1.0**: Fixed temperature parameter
- Smooth exploration-exploitation transition
- Sensitive to Q-value scales

---

## ğŸ“ˆ Key Insights

### Hyperparameter Sensitivity
| Parameter | Low Value Effect | High Value Effect |
|-----------|------------------|-------------------|
| Îµ (Epsilon-Greedy) | Fast convergence, may miss optimal | More exploration, slower learning |
| C (UCB) | Exploitation-focused | Exploration-focused |
| Ï„ (SoftMax) | Greedy behavior | Uniform random |

### Strengths
âœ… Adapts recommendations based on user context  
âœ… Online learning enables continuous improvement  
âœ… Computationally efficient algorithms  
âœ… No large historical datasets required  

### Limitations
âš ï¸ Assumes stationary reward distributions  
âš ï¸ Classification accuracy impacts bandit performance  
âš ï¸ Cold-start problem for new users/categories  

---

## ğŸš€ Production Recommendations

1. **Use UCB** for robust performance without extensive tuning
2. **A/B test** hyperparameters in production
3. **Ensemble methods** for robustness
4. **Periodic retraining** as user preferences evolve

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ lab3_results_U20230081.ipynb  # Main notebook with all code & results
â”œâ”€â”€ README.md                      # Project report (this file)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ news_articles.csv         # News articles dataset
â”‚   â”œâ”€â”€ train_users.csv           # Training user data (with labels)
â”‚   â””â”€â”€ test_users.csv            # Test user data (no labels)
â”œâ”€â”€ assignment.pdf                # Lab assignment specification
â””â”€â”€ Goal.md                       # Assignment requirements reference
```

---

## ğŸ› ï¸ How to Run

```bash
# Install dependencies
pip install rlcmab-sampler numpy pandas matplotlib scikit-learn

# Run the notebook
jupyter notebook lab3_results_U20230081.ipynb
```

Execute all cells top-to-bottom. The notebook includes:
- Data loading & preprocessing
- User classification model training
- Bandit algorithm implementations
- RL simulations (T=10,000 steps)
- Visualization & analysis plots

---

## ğŸ“Œ Conclusion

The **UCB algorithm with C=0.5** achieved the best performance with an average reward of **7.13** over 10,000 steps. The contextual bandit framework successfully balances exploration and exploitation for personalized news recommendations, with algorithm selection depending on computational resources, desired exploration-exploitation trade-off, and real-time performance requirements.
